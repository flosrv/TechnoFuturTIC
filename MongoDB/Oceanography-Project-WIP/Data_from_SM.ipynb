{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from functions import *\n",
    "from IPython.core.display import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_postgresql_creds = r\"C:\\Users\\f.gionnane\\Documents\\Data Engineering\\Credentials\\postgresql_creds.json\"\n",
    "with open(path_postgresql_creds, 'r') as file:\n",
    "    content = json.load(file)\n",
    "    user = content[\"user\"]\n",
    "    password = content[\"password\"]\n",
    "    host = content[\"host\"]\n",
    "    port = content[\"port\"]\n",
    "\n",
    "db = \"MyProjects\"\n",
    "schema = \"End_To_End_Oceanography_ML\"\n",
    "\n",
    "# Créer l'engine PostgreSQL\n",
    "engine = create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}\")\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Available Stations ID List\n",
    "Filter Dysfunctional Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all stations and some metadata as a Pandas DataFrame\n",
    "stations_df = api.stations()\n",
    "# parse the response as a dictionary\n",
    "stations_df = api.stations(as_df=True)\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_error_url_list = []\n",
    "\n",
    "# Liste de mots à rechercher dans la colonne \"Remark\"\n",
    "blacklist = [\"Failure\", \"ceased\", \"failed\", \"recovered\", \"stopped\", 'adrift']\n",
    "stations_id_set = set()\n",
    "\n",
    "print(f'Avant Filtre: {stations_df.shape[0]}')\n",
    "\n",
    "# Liste pour collecter les indices à supprimer\n",
    "indices_a_supprimer = []\n",
    "\n",
    "# Parcours des lignes de la DataFrame\n",
    "for idx, row in stations_df.iterrows():\n",
    "    station_id = row[\"Station\"]\n",
    "    station_Location = row[\"Hull No./Config and Location\"]  # Extraire la valeur de la cellule pour chaque ligne\n",
    "    \n",
    "    # Extraction du nom de la station si un \")\" est trouvé\n",
    "    if \")\" in station_Location:\n",
    "        station_name = station_Location.split(')')[1].rstrip(\" )\")  # On enlève l'espace et la parenthèse en fin de chaîne\n",
    "    else:\n",
    "        station_name = station_Location.strip()  # Si pas de \")\", on garde toute la chaîne\n",
    "\n",
    "    station_name = station_name.rstrip(\" )\").replace(\"(\", \"\").replace(\")\", \"\").strip()\n",
    "\n",
    "    # Nettoyage final pour enlever toute parenthèse ou espace en fin de nom\n",
    "    station_name = station_name.rstrip(\" )\")\n",
    "\n",
    "    # Vérifier si \"Remark\" n'est pas NaN et si un des éléments de blacklist est dans \"Remark\"\n",
    "    if isinstance(row[\"Remark\"], str) and any(blacklist_word.lower() in row[\"Remark\"].lower() for blacklist_word in blacklist):\n",
    "        # Ajouter l'index à la liste\n",
    "        indices_a_supprimer.append(idx)\n",
    "    else:\n",
    "        try:\n",
    "            # Effectuer l'appel API\n",
    "            buoy_data = NDBC.realtime_observations(station_id)\n",
    "            \n",
    "            # Vérifier si les données de l'API sont valides (si le DataFrame n'est pas vide)\n",
    "            if not buoy_data.empty:\n",
    "                print(f'Buoy {station_id}: {station_name} passed the Remarks and API Test!')\n",
    "                stations_id_set.add(station_id)\n",
    "            else:\n",
    "                print(f'Buoy {station_id}: {station_name} did not return valid data. Deleting.')\n",
    "                indices_a_supprimer.append(idx)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Si l'erreur est un HTTPError, on peut essayer d'afficher le code d'erreur\n",
    "            if isinstance(e, HTTPError):\n",
    "                print(f'Buoy {station_id}: {station_name} API Call returned {e.code}. Deleting.')\n",
    "            else:\n",
    "                # Dans tous les autres cas d'exception, on affiche le message d'erreur complet\n",
    "                print(f'Buoy {station_id}: {station_name} API Call encountered an error. Deleting.')\n",
    "                \n",
    "                if str(e).startswith(\"Error accessing\"):\n",
    "                    url = f\"https://www.ndbc.noaa.gov/station_page.php?station={station_id}\"\n",
    "                    access_error_url_list.append([station_id, url])\n",
    "            # Ajouter l'index à la liste en cas d'erreur\n",
    "            indices_a_supprimer.append(idx)\n",
    "\n",
    "# Supprimer les lignes après la boucle\n",
    "stations_df.drop(index=indices_a_supprimer, inplace=True)\n",
    "\n",
    "print(f'Après Filtre: {stations_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in access_error_url_list:\n",
    "    print(f\"Access error for buoy {item[0]}\")\n",
    "    print(f\"{item[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_id_list = list(stations_id_set)\n",
    "\n",
    "for item in stations_id_list:\n",
    "    metadata = get_station_metadata(item)\n",
    "    station_name, station_id, station_zone, lat_buoy, lon_buoy, marine_data_table_name = parse_buoy_json(metadata)\n",
    "    print(f\"Name: {station_name}\")\n",
    "    print(f\"Station ID: {station_id}\")\n",
    "    print(f\"Lat: {lat_buoy}\")\n",
    "    print(f\"Lon: {lon_buoy}\")\n",
    "    print(f\"Zone: {station_zone}\")\n",
    "    print(f\"Base Table Name: {marine_data_table_name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = get_station_metadata(\"FFIA2\")\n",
    "station_name, station_id, station_zone, lat_buoy, lon_buoy, marine_data_table_name = parse_buoy_json(metadata)\n",
    "print(station_zone)\n",
    "for key, value in metadata.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_dict = {}\n",
    "\n",
    "for station in stations_id_list:\n",
    "    metadata = get_station_metadata(station)  # Récupérer les métadonnées de la station\n",
    "\n",
    "    # Assurez-vous que les métadonnées sont correctement récupérées avant de les analyser\n",
    "    if metadata:\n",
    "        # Parser les métadonnées de la station\n",
    "        station_name, station_id, station_zone, lat_buoy, lon_buoy, marine_data_table_name = parse_buoy_json(metadata)\n",
    "        stations_dict[station_id] = {}\n",
    "\n",
    "        # Remplir le dictionnaire avec les données traitées\n",
    "        stations_dict[station_id][\"Station Name\"] = station_name\n",
    "        stations_dict[station_id][\"Lat\"] = lat_buoy\n",
    "        stations_dict[station_id][\"Lon\"] = lon_buoy\n",
    "        stations_dict[station_id][\"Zone\"] = station_zone\n",
    "        stations_dict[station_id][\"Table Name\"] = marine_data_table_name\n",
    "\n",
    "        # Affichage pour chaque station\n",
    "        print(f\"Name: {station_name}\")\n",
    "        print(f\"Station ID: {station_id}\")\n",
    "        print(f\"Lat: {lat_buoy}\")\n",
    "        print(f\"Lon: {lon_buoy}\")\n",
    "        print(f\"Zone: {station_zone}\")\n",
    "        print(f\"Base Table Name: {marine_data_table_name}\\n\")\n",
    "\n",
    "# Résultat final du dictionnaire des stations\n",
    "stations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_with_flush(message):\n",
    "    sys.stdout.write(f'\\r{message}  ')  # \\r permet de revenir au début de la ligne\n",
    "    sys.stdout.flush()  # Force l'affichage immédiat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Bronze Layer Table Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_id_list =list(stations_id_set)\n",
    "buoy_chosen = random.choice(stations_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buoy_chosen_metadata = get_station_metadata(buoy_chosen)\n",
    "buoy_chosen_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation avec un dictionnaire 'buoy_chosen_metadata'\n",
    "try:\n",
    "    lat_buoy, lon_buoy, station_name, station_id, station_zone, marine_data_table_name = parse_buoy_json(buoy_chosen_metadata)\n",
    "    print(lat_buoy, lon_buoy, station_name, station_id, station_zone, marine_data_table_name)\n",
    "except ValueError as e:\n",
    "    print(f\"Erreur lors du traitement des données: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marine = NDBC.realtime_observations(buoy_chosen)\n",
    "print(type(buoy_chosen_metadata))\n",
    "df_marine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marine API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Data From Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Remplacer `{station_id}` par l'identifiant de la station spécifique\n",
    "url = f\"https://www.ndbc.noaa.gov/station_page.php?station={station_id}\"\n",
    "\n",
    "# Faire une requête GET pour obtenir le HTML de la page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier que la requête a réussi\n",
    "if response.status_code == 200:\n",
    "    # Parse le HTML avec BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Trouver la division avec l'ID 'stationmetadata'\n",
    "    station_metadata = soup.find(id=\"stationmetadata\")\n",
    "    \n",
    "    # Vérifier si la division existe\n",
    "    if station_metadata:\n",
    "        # Chercher les deux images spécifiques\n",
    "        img_1 = station_metadata.find('img', src='/images/stations/3mfoam_scoop_mini.jpg')\n",
    "        img_2 = station_metadata.find('img', src='/images/buoycam/W64A_2025_03_15_1510.jpg')\n",
    "\n",
    "        # Si l'image 1 est trouvée, modifier son lien en absolu\n",
    "        if img_1:\n",
    "            img_1['src'] = urljoin(url, img_1['src'])\n",
    "\n",
    "        # Si l'image 2 est trouvée, modifier son lien en absolu\n",
    "        if img_2:\n",
    "            img_2['src'] = urljoin(url, img_2['src'])\n",
    "        \n",
    "        # Afficher directement le HTML avec les liens des images mis à jour\n",
    "        display(HTML(str(station_metadata)))  # Affiche la division en HTML rendu\n",
    "    else:\n",
    "        print(\"La division avec l'ID 'stationmetadata' n'a pas été trouvée.\")\n",
    "else:\n",
    "    print(f\"Erreur lors de la récupération de la page, statut: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Stations from Caribbean Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du nom de la table\n",
    "bronze_meteo_data_table_name = f\"bronze_meteo_data_{station_name.replace(' ', '_')}_{station_zone.replace(' ', '_')}_{str(lat_buoy).replace('.', '-')}_{str(lon_buoy).replace('.', '-')}\"\n",
    "bronze_meteo_data_table_name = bronze_meteo_data_table_name.replace('.', '-')\n",
    "load_data_in_table(db=db, schema=schema, table_name=bronze_meteo_data_table_name, df=df_meteo,conn=conn,key_column='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"buoy_near_SM.json\", \"r\") as f:\n",
    "    buoy_near_SM = json.load(f)\n",
    "\n",
    "\n",
    "path_postgresql_creds = r\"C:\\Users\\f.gionnane\\Documents\\Data Engineering\\Credentials\\postgresql_creds.json\"\n",
    "with open(path_postgresql_creds, 'r') as file:\n",
    "    content = json.load(file)\n",
    "    user = content[\"user\"]\n",
    "    password = content[\"password\"]\n",
    "    host = content[\"host\"]\n",
    "    port = content[\"port\"]\n",
    "\n",
    "# Créer l'engine PostgreSQL\n",
    "engine = create_engine(f\"postgresql+psycopg2://{user}:{password}@{host}:{port}/{db}\")\n",
    "conn = engine.connect()\n",
    "\n",
    "db = buoy_near_SM[\"db\"]\n",
    "schema = buoy_near_SM[\"schema\"] \n",
    "\n",
    "bronze_marine_data_table_name = buoy_near_SM[\"bronze_marine\"] \n",
    "bronze_meteo_data_table_name = buoy_near_SM[\"bronze_meteo\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_marine_to_clean = fetch_table_data(conn=conn, schema=schema, table_name= bronze_marine_data_table_name)\n",
    "    df_meteo_to_clean = fetch_table_data(conn=conn, schema=schema, table_name= bronze_meteo_data_table_name)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_convert(df):\n",
    "    for col in df.columns:\n",
    "        # Essayer de convertir en datetime\n",
    "        if df[col].dtype == 'object':\n",
    "            try:\n",
    "                # Si tu connais le format, tu peux spécifier ici, par exemple '%Y-%m-%d'\n",
    "                # Exemple de format: '2021-01-01' ou '01/01/2021'\n",
    "                df[col] = pd.to_datetime(df[col], format='%Y-%m-%d', errors='raise')  # Converte en datetime\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # Essayer de convertir en numérique\n",
    "        if df[col].dtype == 'object':\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='raise')  # Converte en numérique\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return df\n",
    "\n",
    "df_marine_to_clean = auto_convert(df_marine_to_clean)\n",
    "print(df_marine_to_clean.dtypes)\n",
    "df_marine_to_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marine_to_clean = handle_null_values(df_marine_to_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_to_clean = auto_convert(df_meteo_to_clean)\n",
    "df_meteo_to_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_marine_to_clean = handle_null_values(df_marine_to_clean)\n",
    "df_meteo_to_clean = handle_null_values(df_meteo_to_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_marine_to_clean.columns)\n",
    "print(df_meteo_to_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_to_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open-meteo API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo_to_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo = drop_columns_if_exist(df_meteo,['rain', 'showers','soil_moisture_0_to_1cm', 'cloud_cover', 'soil_temperature_0cm',\t'soil_moisture_0_to_1cm', 'is_day'])\n",
    "df_meteo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meteo.rename(columns={'temperature_2m': 'T°(C°)', \n",
    "                         'relative_humidity_2m': 'Relative Humidity (%)',\n",
    "                         'dew_point_2m': 'Dew Point (°C)', \n",
    "                         'precipitation': 'Precipitation (mm)', \n",
    "                         'pressure_msl':' Sea Level Pressure (hPa)', \n",
    "                         'cloud_cover_low':'Low Clouds (%)',\n",
    "                         'cloud_cover_mid' : 'Middle Clouds (%)',\t\n",
    "                         'cloud_cover_high' : 'High Clouds (%)', \n",
    "                         'visibility' : ' Visibility (%)', \n",
    "                         'wind_speed_10m' : 'Wind Speed (km/h)'}, \n",
    "                         inplace=True)\n",
    "df_marine.rename(columns={\n",
    "    'wind_direction': 'Wind Direction (°)',\n",
    "    'wind_speed': 'Wind Speed (km/h)',\n",
    "    'wind_gust': 'Wind Gusts (km/h)',\n",
    "    'wave_height': 'Wave Height (m)',\n",
    "    'average_wave_period': 'Average Wave Period (s)',\n",
    "    'dominant_wave_direction': 'Dominant Wave Direction (°)',\n",
    "    'pressure': 'Pressure (hPA)',\n",
    "    'air_temperature': 'Air T°',\n",
    "    'water_temperature': 'Water T°'}, \n",
    "    inplace=True)\n",
    "\n",
    "print(df_meteo.columns)\n",
    "print(df_marine.columns)\n",
    "print(df_marine.shape)\n",
    "print(df_meteo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effectuer la jointure interne sur la colonne 'time'\n",
    "df_merged = pd.merge(df_marine, df_meteo, on = 'Datetime', how='inner')\n",
    "\n",
    "# Afficher le résultat\n",
    "print(df_merged.shape)\n",
    "print(df_merged.dtypes)\n",
    "df_merged.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation\n",
    "df_merged = add_daytime_and_month_column(df_merged,'Datetime')\n",
    "\n",
    "print(df_merged.columns)\n",
    "df_merged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Wind Speed (km/h)'] = (df_merged['Wind Speed (km/h)_x']+ df_merged['Wind Speed (km/h)_y'])/2\n",
    "df_merged = drop_columns_if_exist(df_merged, ['Wind Speed (km/h)_x', 'Wind Speed (km/h)_y', 'Wind Gusts (km/h)'])\n",
    "\n",
    "print(df_merged.columns)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connexion BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.oauth2 import service_account\n",
    "# from google.cloud import storage  # Exemple pour Google Cloud Storage\n",
    "# from google.cloud import bigquery\n",
    "# from google.cloud.exceptions import NotFound\n",
    "# import pandas as pd\n",
    "# import pyarrow\n",
    "\n",
    "# path_to_google_creds = r\"C:\\Users\\f.gionnane\\Documents\\Data Engineering\\Credentials\\google_credentials.json\"\n",
    "\n",
    "# bq_client = bigquery.Client.from_service_account_json(\n",
    "#     path_to_google_creds)\n",
    "# project_id = \"rare-bloom-419220\"\n",
    "# dataset_End_To_End_Oceanography_ML = \"End_To_End_Oceanography_ML\"\n",
    "# bq_client\n",
    "# # dataset_ref = bq_client.dataset('my_dataset_name', project=project_id)\n",
    "\n",
    "\n",
    "# # LIST DATASETS AND FIND ONE\n",
    "# datasets = list(bq_client.list_datasets())  # Make an API request.\n",
    "# project = client.project\n",
    "# bq_datasets_list =[]\n",
    "\n",
    "# if datasets:\n",
    "#     print(\"Datasets in project {}:\".format(project))\n",
    "#     for dataset in datasets:\n",
    "#         print(\"\\t{}\".format(dataset.dataset_id))\n",
    "#         bq_datasets_list.append(dataset.dataset_id)\n",
    "#     if dataset_End_To_End_Oceanography_ML in bq_datasets_list:\n",
    "#         dataset =  dataset_End_To_End_Oceanography_ML\n",
    "#         print(\"Dataset Found !\")\n",
    "# else:\n",
    "#     print(\"{} project does not contain any datasets.\".format(project))\n",
    "\n",
    "# # (developer): Set table_id to the ID of the table to determine existence.\n",
    "# # table_id = \"your-project.your_dataset.your_table\"\n",
    "\n",
    "# try:\n",
    "#     table_ref = bq_client.dataset(dataset).table(table_name)\n",
    "#     bq_client.get_table(table_ref)  # Make an API request.\n",
    "#     print(\"Table {} already exists.\".format(table_name))\n",
    "# except NotFound:\n",
    "#     print(\"Table {} is not found.\".format(table_name))\n",
    "\n",
    "\n",
    "# def clean_column_names(df):\n",
    "\n",
    "#     cleaned_columns = []\n",
    "#     for column in df.columns:\n",
    "#         # Remplacer tous les caractères non alphanumériques (sauf underscores) par un underscore\n",
    "#         cleaned_column = re.sub(r'[^A-Za-z0-9_]', '_', column)\n",
    "        \n",
    "#         # Ajouter le nom de colonne nettoyé à la liste\n",
    "#         cleaned_columns.append(cleaned_column)\n",
    "    \n",
    "#     # Appliquer les nouveaux noms de colonnes au DataFrame\n",
    "#     df.columns = cleaned_columns\n",
    "#     return df\n",
    "\n",
    "# df_merged = clean_column_names(df_merged)\n",
    "\n",
    "# table_id = f\"{project_id}.{dataset}.{table_name}\"\n",
    "\n",
    "\n",
    "# try:\n",
    "#     bq_client.get_table(table_id)  # Make an API request.\n",
    "#     print(\"Table {} already exists.\".format(table_id))\n",
    "# except NotFound:\n",
    "#     print(\"Table {} is not found.\".format(table_id))\n",
    "#     bq_client.create_table(table_id)\n",
    "#     print(\"Creation of the Table {}.\".format(table_id))\n",
    "\n",
    "\n",
    "# def load_data_to_bigquery(client, dataset: str = None, table: str = None, df: pd.DataFrame = None, key_column: str = 'Datetime', table_id: str = None):\n",
    "   \n",
    "    \n",
    "#     # Fonction pour détecter et convertir les types de données\n",
    "#     def convert_column_types(df):\n",
    "#         for column in df.columns:\n",
    "#             dtype = df[column].dtype\n",
    "\n",
    "#             if dtype == 'object':  # Chaînes de caractères\n",
    "#                 df[column] = df[column].astype(str)\n",
    "#             elif dtype == 'datetime64[ns]':  # Datetime\n",
    "#                 df[column] = pd.to_datetime(df[column], errors='coerce').dt.tz_localize('UTC', ambiguous='NaT').dt.tz_localize(None)\n",
    "#             elif dtype == 'float64':  # Float\n",
    "#                 df[column] = df[column].astype('float32')\n",
    "#             elif dtype == 'int64':  # Integer\n",
    "#                 df[column] = df[column].astype('int64')  # On garde int64, car BigQuery supporte ce type\n",
    "#             else:\n",
    "#                 # Autres types, on les convertit en string\n",
    "#                 df[column] = df[column].astype(str)\n",
    "        \n",
    "#         return df\n",
    "\n",
    "#     # Convertir les types des colonnes\n",
    "#     df = convert_column_types(df)\n",
    "\n",
    "#     if table_id:\n",
    "#         # Si table_id est fourni, on l'utilise directement.\n",
    "#         full_table_id = table_id\n",
    "#     elif dataset and table:\n",
    "#         # Si table_id n'est pas fourni, on construit table_id à partir de dataset et table.\n",
    "#         full_table_id = f\"{client.project}.{dataset}.{table}\"\n",
    "#     else:\n",
    "#         raise ValueError(\"Il faut fournir soit 'table_id' ou les paramètres 'dataset' et 'table' séparés.\")\n",
    "\n",
    "#     # Vérifier si le dataset existe, sinon le créer\n",
    "#     try:\n",
    "#         dataset = full_table_id.split('.')[1]\n",
    "#         client.get_dataset(dataset)  # Vérifie si le dataset existe\n",
    "#         print(f\"Le dataset {dataset} existe déjà.\")\n",
    "#     except NotFound:\n",
    "#         print(f\"Le dataset {dataset} n'existe pas. Création du dataset...\")\n",
    "#         client.create_dataset(dataset)  # Crée le dataset s'il n'existe pas\n",
    "#         print(f\"Le dataset {dataset} a été créé.\")\n",
    "\n",
    "#     # Vérifier si la table existe, sinon la créer\n",
    "#     try:\n",
    "#         client.get_table(full_table_id)  # Vérifie si la table existe\n",
    "#         print(f\"La table {full_table_id} existe déjà.\")\n",
    "#     except NotFound:\n",
    "#         print(f\"La table {full_table_id} n'existe pas. Création de la table...\")\n",
    "#         # Créer la table avec le schéma du DataFrame\n",
    "#         schema = []\n",
    "#         for name, dtype in df.dtypes.items():\n",
    "#             if name == 'Datetime':\n",
    "#                 schema.append(bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.TIMESTAMP))\n",
    "#             elif dtype == 'float32' or dtype == 'float64':\n",
    "#                 schema.append(bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.FLOAT64))\n",
    "#             elif dtype == 'int64':\n",
    "#                 schema.append(bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.INTEGER))\n",
    "#             else:\n",
    "#                 schema.append(bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING))\n",
    "        \n",
    "#         # Créer la table avec le schéma\n",
    "#         table = bigquery.Table(full_table_id, schema=schema)\n",
    "#         client.create_table(table)  # Crée la table si elle n'existe pas\n",
    "#         print(f\"La table {full_table_id} a été créée.\")\n",
    "\n",
    "#     # Préparer les données pour l'insertion\n",
    "#     if key_column in df.columns:\n",
    "#         # Si la colonne clé est fournie, supprimer les doublons en fonction de cette colonne\n",
    "#         df = df.drop_duplicates(subset=[key_column])\n",
    "\n",
    "#     # Charger les données dans la table BigQuery\n",
    "#     job_config = bigquery.LoadJobConfig(\n",
    "#         schema=[\n",
    "#             bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.TIMESTAMP) if name == 'Datetime' else\n",
    "#             bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.FLOAT64) if dtype == 'float32' or dtype == 'float64' else\n",
    "#             bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.INTEGER) if dtype == 'int64' else\n",
    "#             bigquery.SchemaField(name, bigquery.enums.SqlTypeNames.STRING)\n",
    "#             for name, dtype in df.dtypes.items()\n",
    "#         ],\n",
    "#         write_disposition=\"WRITE_APPEND\"  # Ajoute les données sans écraser les anciennes\n",
    "#     )\n",
    "\n",
    "#     # Charger le DataFrame dans BigQuery\n",
    "#     job = client.load_table_from_dataframe(df, full_table_id, job_config=job_config)\n",
    "#     job.result()  # Attendre la fin de la tâche\n",
    "#     print(f\"Données chargées dans la table {full_table_id}.\")\n",
    "\n",
    "# # Exemple d'appel à la fonction\n",
    "# load_data_to_bigquery(table_id=table_id, client=bq_client, df=df_merged)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
